---
title: 支持向量机
date: 2018-08-23 14:47:29
categories: 
- 数据挖掘
tags: ["支持向量机","sklearn"]
---
# 引言


本文主要使用python机器学习包scikit-learn，使用UCI上的机器学习数据Iris进行分类。

# 1 数据集

数据集是来自UCI机器学习数据仓库中的Iris数据，该数据包含4个特征，分别是sepal length,sepal width, petal length,petal width,总共有3类，分别是Iris Setosa, Iris Versicolour, Iris Virginica，每一类数据有50条样本，共计150条样本数据。部分数据如下所示：

5.1,3.5,1.4,0.2,Iris-setosa  
4.9,3.0,1.4,0.2,Iris-setosa  
4.7,3.2,1.3,0.2,Iris-setosa  
4.6,3.1,1.5,0.2,Iris-setosa  
5.0,3.6,1.4,0.2,Iris-setosa  

# 2 开始编码

## 2.1 导入数据

使用pandas进行数据的读取，首先将数据文件后缀名修改为csv，其中第一行会被默认的作为列名，有两种解决方法，一种是在数据中第一行添加对应的列名,另一种可以在read_csv中使用header=-1来从第一行读取数据。本文决定在数据集中添加列名。


```python
import pandas as pd
import numpy as np
import sklearn
from sklearn.model_selection import train_test_split
from sklearn.svm import SVC
import matplotlib.pyplot as plt

FILEPATH = r"C:\others\doc\data mining\Iris\Iris.csv"

```


```python
datasets = pd.read_csv(open(FILEPATH),sep=",");
```

对数据进行预处理，将class列中的字符串对应的转换成数字。  
将数据转化成二维列表，便于处理，转化之后的数据就直接是numpy的ndarray格式的数组，可以进行切分等。


```python
def type_change(s):
    it = {"Iris-setosa":0, "Iris-versicolor":1,"Iris-virginica":2}
    return it[s]

# 对数据集的class列进行处理，得到类型转换之后的数据集。通过查看官方文档，可以找到converters参数进行转换。
datasets = pd.read_csv(open(FILEPATH),sep=",",converters={4:type_change})
datasets = datasets.values
print(type(datasets))
```

    <class 'numpy.ndarray'>
    

## 2.2 将数据切分成训练器和测试集

使用scitkie-learn中的一个方法，可以随机生成测试集和训练集。
sklearn.model_selection.train_test_split(train_data, train_target, test_size, train_size, random_state)
- test_size 默认情况下为0.25,可以为浮点数或者int型，在一致train_size的时候，它就确定了
- random_state 随机数种子，其实就是改组随机数的编号，在需要重复试验的时候，保证得到一组一样的随机数。比如每次都填1，其他参数设置一样的情况下得到的随机数组是一样的，但是如果填0或者不填，每次都会不一样。随机数的产生取决于中农资；随机数和种子之间的关系遵从以下两个原则：种子不同，产生不同的随机数；种子相同，即使实例不同也产生相同的随机数。

返回值跟传入的数组有关，如果有x，y，返回的就是2*len(x,y) = 2*x = 4。



```python
x,y = np.split(datasets, (4,), axis=1)    # 将数据集的特征和标签分割开来

x = x[:, :2]   # 这里取前面两个属性特征值来训练，是为了后面画图可以比较直观

x_train, x_test, y_train, y_test = train_test_split(x, y, random_state=1, train_size=0.6)
```    

## 2.3 训练SVM分类器

class sklearn.svm.SVC(C=1.0, kernel=’rbf’, degree=3, gamma=’auto’, coef0=0.0, shrinking=True, probability=False, tol=0.001, cache_size=200, class_weight=None, verbose=False, max_iter=-1, decision_function_shape=’ovr’, random_state=None)
官方文档：
c-支持向量分类：该实现是基于libsvm，拟合事件复杂度大于样本数量的平方，因此当样本数量大于10000的时候就不适用了。同时它的多类支持是通过一对一的方案解决的。
[全面的支持向量机文档](http://scikit-learn.org/stable/modules/svm.html#svm-classification)

- C 惩罚参数
- kernel 内核函数，可以是linear  poly  rbf  sigmoid  precomputed 分别是线性核函数、多项式核函数（最常用的径向机核函数就是高斯核函数）、径向机核函数、神经元的非线性作用函数核函数、用户自定义核函数。
- degree 当使用多项式核函数的时候，degree定义了该核函数的多项式次数。
- gamma rbf  poly  sigmoid的核系数。【具体含义不清楚】
- decision_function_shape   ovr就是one vs rest  一个类别与其他类别进行划分     ovo  one vs one类别两两之间进行划分，即使用二分类的方法模拟多分类的结果。


```python
classfication = SVC(C=0.8, kernel='rbf', gamma=20, decision_function_shape='ovr')

# 根据给定的训练数据训练拟合模型,这里的y_train需要转换成一个一维的数组
classfication.fit(x_train, y_train.ravel())

```




    SVC(C=0.8, cache_size=200, class_weight=None, coef0=0.0,
      decision_function_shape='ovr', degree=3, gamma=20, kernel='rbf',
      max_iter=-1, probability=False, random_state=None, shrinking=True,
      tol=0.001, verbose=False)



### 2.3.1 关于分类之后的一些参数

- 支持向量


```python
# 支持向量
classfication.support_vectors_

# 支持向量的索引
classfication.support_

# 支持向量的个数
classfication.n_support_
```




    array([19, 27, 27])



## 2.4 计算分类器的准确率

score(x,yweights=None) 返回给定数据x和标签y的平均准确率



```python
train_accuray = classfication.score(x_train,y_train)
test_accuracy = classfication.score(x_test, y_test)
print("训练集准确率为:%.5f, 测试集准确率为：%.5f" % (train_accuray, test_accuracy))
```

    训练集准确率为:0.87778, 测试集准确率为：0.81667
    

predict(x),呈现分类的结果。只用给数据即可。


```python
classfication.predict(x_test)
```




    array([0., 1., 2., 0., 2., 2., 2., 0., 0., 2., 1., 0., 2., 2., 1., 0., 1.,
           1., 0., 0., 1., 1., 2., 0., 2., 1., 0., 0., 1., 1., 2., 2., 1., 2.,
           1., 0., 1., 0., 2., 1., 2., 0., 1., 2., 2., 2., 0., 0., 0., 1., 0.,
           0., 2., 2., 2., 2., 2., 1., 2., 1.])



decision_function(x) 可以查看样本到分离超平面的距离,通过与上面的predict结果对比发现，所属类别为距离最大的那一类，这是为什么？

decision_function()  为每个样本提供每个类别的分数。


```python
classfication.decision_function(x_test)[:4]
```




    array([[ 2.08923718,  0.95436859, -0.04360578],
           [-0.31687426,  2.31494218,  1.00193208],
           [-0.47479005,  1.01791306,  2.456877  ],
           [ 2.25585785,  0.87236638, -0.12822423]])



## 2.5 绘制图像


```python
# 画训练数据的图形
plt.figure()

x = x_train[:,:1].ravel()
y = x_train[:,1:2].ravel()

labels = y_train
for i in range(len(x)):
    if labels[i] == 0:
        plt.scatter(x[i],y[i], color='red')
    if labels[i] == 1:
        plt.scatter(x[i],y[i],color="g")
    if labels[i] == 2:
        plt.scatter(x[i],y[i], color="k")
        
plt.xlabel('sepal length')
plt.ylabel('sepal width')
plt.title('result')
plt.show()
```


<img src="/images/DataMiningTheory/SVM/scatter1.png">

使用color列表，就是根据标签决定不同的颜色，然后进行绘制，绘制结果和上图一样。


```python
plt.figure()

x = x_train[:,:1].ravel()
y = x_train[:,1:2].ravel()

labels = y_train
color = [];
for i in range(len(labels)):
    if labels[i] == 0:
        color.append('r')
    if labels[i] == 1:
        color.append('g')
    if labels[i] == 2:
        color.append('k')
plt.scatter(x,y,c=color)
        
plt.xlabel('sepal length')
plt.ylabel('sepal width')
plt.title('result')
plt.show()

```

<img src="/images/DataMiningTheory/SVM/scatter1.png">

# 3 总结

目前暂时只是简单的使用sklearn中的SVM分类器对UCI中的Iris数据进行分类。之后还需要了解SVM的其他具体参数的设置以及其他的一些，比如回归等。可以参考的内容：
[svm](http://scikit-learn.org/stable/modules/svm.html)
[svc](http://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html)
[plot svm](http://scikit-learn.org/stable/auto_examples/svm/plot_iris.html#sphx-glr-auto-examples-svm-plot-iris-py)
