---
title: '聚类'
date: 2018-09-13 08:27:01
categories:
- 数据挖掘
tags:
- 算法
- k-means
---


# 引言
聚类是一种非监督学习，k-means聚类是一种常用的聚类算法，本文将通过Python实现k-means聚类。具体比较详细的理论介绍这里就不进行说明，可以参考《机器学习》——周志华中的第九章，本文更注重实现。

# 1 k-means聚类算法步骤
首先，我们有一些数据x1,x2,x3,...xn和聚类簇数K。
1. 随机选择K个点作为聚类的中心
2. 通过计算每个点和聚类中心的距离，从而将改变分配到距离最小的聚类簇中
3. 计算新的聚类中心，通过计算聚类簇中点的平均值
4. 重复2和3直到任何群集分配没有改变

# 2 使用python实现
首先，加载一些python包和读取数据
```python
from copy import deepcopy
import numpy as np
import pandas as pd
from matplotlib import pyplot as plt
plt.rcParams['figure.figsize'] = (16,9)
plt.style.use('ggplot')

dataset_filepath = r'C:\others\doc\data mining\xclara.csv'
```
```python
# importing the dataset
data = pd.read_csv(open(dataset_filepath))
print(data.shape)
data.head()
```
(3000,2)

|      | V1        |    v2     |
| ---- | --------- | --------- |
| 0    | 2.072345  | -3.241693 |
| 1    | 17.936710 | 15.784810 |
| 2    | 1.083576  | 7.319176  |
| 3    | 11.120670 | 14.406780 |
| 4    | 23.711550 | 2.557729  |

因为数据是二维的，可以绘制出图形看一下。
```python
# plot scatter
f1 = data['V1'].values
f2 = data['V2'].values
X = np.array(list(zip(f1,f2)))
plt.scatter(f1, f2, c='k', s=7)
plt.show()
```
<img src="/images/DataMiningTheory/Cluster/scatter_01.png" width="600px" height="400px">

接着，定义欧几里得距离公式计算方法
```python
# Euclidean Distance Caculator
def dist(a, b, ax=1):
    return np.linalg.norm(1-b,axis=ax)
```

初始化聚类簇的中心
```python
# cluster numbers
k = 3
# random centroid of X
c_x = np.random.randint(0, np.max(X)-20, size=k)
# random centroid of Y
c_y = np.random.randint(0, np.max(X)-20,size=k)
C = np.array(list(zip(c_x,c_y)), dtype=np.float32)
```
在图中显示出数据点和初始聚类中心的位置
```python
plt.scatter(f1, f2,c='k',s=7)
plt.scatter(c_x, c_y, c='r', s=200, marker='*')
```
<img src="/images/DataMiningTheory/Cluster/scatter_02.png" width="600px" height="400px">

关键代码，迭代直到聚类簇分配没有变化的时候停止
```python
# store the value of centroids when it updates
c_old = np.zeros(C.shape)
#cluster Lables(0,1,2), to store the cluster label of each sample point 
clusters = np.zeros(len(X))

# error -- distance between new centroids and old centroids
error = dist(C, c_old, None)
print(error)

# Loop will run until the error becomes zero
while error != 0:
    # Assigning each point to its closest cluster
    for i in range(len(X)):
        distances = dist(X[i], C)
        cluster = np.argmin(distances)
        clusters[i] = cluster
    
    #store the old centroid values
    c_old = deepcopy(C)
    
    # find the new centroids by taking the average value
    for i in range(k):
        points = [X[j] for j in range(len(X)) if clusters[j] == i]
        C[i] = np.mean(points, axis=0)
    
    error = dist(C, c_old, None)
    print(error)
```
做出最后的结果图形，主要是包括不同样本点的颜色显示和聚类簇中心显示
```python
# plot scatter and the centroids of clusters
colors = ['r','g','b','k','c']
fig, ax = plt.subplots()
for i in range(k):
    points = np.array([X[j] for j in range(len(X)) if clusters[j] == i])
    ax.scatter(points[:,0], points[:,1], s= 7, c=colors[i])

ax.scatter(C[:,0],C[:,1], marker="*", c='#050505', s=200)
```
<img src="/images/DataMiningTheory/Cluster/scatter_03.png" width="600px" height="400px">

# 3 参考链接
- http://python.jobbole.com/88535/   
- https://www.datacamp.com/community/tutorials/k-means-clustering-python   
- https://mubaris.com/2017/10/01/kmeans-clustering-in-python/



