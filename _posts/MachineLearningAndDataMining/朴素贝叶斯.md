---
title: 朴素贝叶斯分类代码实现
date: 2018-08-07 16:17:51
categories: 
- 数据挖掘
tags: ["贝叶斯","分类"]
toc: true
comments: true
---
# 朴素贝叶斯

朴素贝叶斯分类的流程可以看下图。  

<img src="/images/DataMiningTheory/BayesAndDecesionTree/Bayes_01" width="400px;" height="400px">


本文主要是通过代码一步步实现一个高斯朴素贝叶斯分类程序。

# 1 数据预处理

## 1.1 读取和处理数据


```python
import csv
import math
import random
import copy

filename = r'C:\Users\14259\Desktop\23周\wine.csv'
```


```python
# 1、导入csv文件
csv_file = csv.reader(open(r'C:\Users\14259\Desktop\23周\wine.csv','r'))
print(csv_file)  # 这里是一个csv.reader的对象
for item in csv_file:
    print(item)
    # 这里没有输出对应的item，因为数据较多。
```

    <_csv.reader object at 0x000001EF4B0DEA70>

```python
# 将读取文件内容封装成方法，存到二维数组中
def loadCsvFile(filename):
    csv_reader = csv.reader(open(filename,'r'))
    dataset = list(csv_reader)
#     print(dataset)
    # 将数据类型由字符串转换成浮点数
    for i in range(len(dataset)):
        dataset[i] = [float(x) for x in dataset[i]]
    return dataset
    
dataset = loadCsvFile(filename)
print("loaded data file {0} with {1} lines".format (filename,len(dataset)))
```

    输出：loaded data file C:\Users\14259\Desktop\23周\wine.csv with 178 lines


## 1.2 将数据集进行划分

将数据集随机分为包含67%的训练集和33%的测试集（这是在次数据集上测试算法的通常比率）


```python
def splitDataset(dataset,splitRatio):
    # splitRation 是训练数据占得比例
    data= copy.deepcopy(dataset)
    length = len(dataset)
    trainSize= math.floor(length*splitRatio)
    trainData = []
    while(len(trainData)<trainSize):
        index = random.randint(0,len(data)-1)
        trainData.append(data.pop(index))
    testData = data
    return [trainData,testData]

trainData,testData = splitDataset(dataset,0.67)
# print(len(trainData),len(testData))
print("train data is %d lines and test data is %d lines" % (len(trainData),len(testData)))
# print(trainData)
```

    输出：train data is 119 lines and test data is 59 lines


## 1.3 提取数据特征


所收集的训练数据的特征，包含相对于每个类的每个属性的均值和标准差。像本数据集，共有3个类别，13个属性，然后我们需要每一个属性和类别组合的均值和标准差，也就是39个属性特征。  

需要完成以下几个任务
- 按照类别进行划分，本数据集共含有三个类别，可以划分成对应的三组数据
- 计算均值、标准差
- 提取属性特征

### 1.3.1 类别划分


```python
# 将类别作为键，对应类别的值等作为改建对应的值
# 1、按照类别进行数据分类
def separateByClass(dataset):
    dataClass = {}
    for i in range(len(dataset)):
        item = dataset[i]  
        if(item[0] not in dataClass.keys()):  # 如果没有该键值
            dataClass[item[0]] = []
        dataClass[item[0]].append(item)
    return dataClass

# dataClass = separateByClass(trainData)
# # print(dataClass)
# print("class 1 has %d lines, class 2 has %d lines, class 3 has %d lines" % (len(dataClass[1.0]), len(dataClass[2.0]), len(dataClass[3.0])))
```

### 1.3.2 计算均值和标准差


```python
# numbers是要计算均值的数据列表
def mean(numbers):
    return sum(numbers) / float(len(numbers));

def stdev(numbers):
    avg = mean(numbers)
    variance = sum([pow(x-avg,2) for x in numbers])/(float(len(numbers)-1))  # 这里使用len(numbers)-1,可以参考百度百科【标准差】的说法
    return math.sqrt(variance)

```

### 1.3.3 提出属性特征

接着就是针对每一个类别，计算每一个属性的均值和标准差。


```python
# 使用zip，可以将对应的属性整合到一个元祖中，然后在进行计算。
test = [[1,20,0],[2,21,0],[3,22,1]]
print(*test)   # 将列表拆分成单个元素进行传输

# 将列表中对应的项组合起来，形成元组
print(list(zip(*test)))
```

    输出：[1, 20, 0] [2, 21, 0] [3, 22, 1]
    [(1, 2, 3), (20, 21, 22), (0, 0, 1)]



```python
def summrize(dataset):
    summaries = [(mean(attributes),stdev(attributes)) for attributes in zip(*dataset)]
    del summaries[0]   # 第一列表示的是类别列，不需要，可以删除
    return summaries

# 可以得到对应每个属性的均值和标准差
# dataset = [[2.0, 13.05, 3.86, 2.32, 22.5, 85.0, 1.65, 1.59, 0.61, 1.62, 4.8, 0.84, 2.01, 515.0], [2.0, 12.16, 1.61, 2.31, 22.8, 90.0, 1.78, 1.69, 0.43, 1.56, 2.45, 1.33, 2.26, 495.0], [2.0, 12.0, 1.51, 2.42, 22.0, 86.0, 1.45, 1.25, 0.5, 1.63, 3.6, 1.05, 2.65, 450.0], [2.0, 11.62, 1.99, 2.28, 18.0, 98.0, 3.02, 2.26, 0.17, 1.35, 3.25, 1.16, 2.96, 345.0]]
# summrize(dataset)

```


```python
# 开始按照类别提取属性特征
def summrizeByClass(dataset):
    seperateData = separateByClass(dataset)      # 获取分类之后的数据
    summaries = {}
    for key,value in seperateData.items():
        summaries[key] = summrize(value)
    return summaries

# # 得到每个类别的均值和标准差
# dataset = loadCsvFile(filename)
# summrizeByClass(dataset)
summaries = summrizeByClass(trainData)
# print(summaries)
```

# 2 预测

现在开始计算归属每个类的概率
- 计算高斯概率密度函数
- 计算对应类的概率
- 单一预测
- 评估

## 2.1 计算高斯概率密度函数


```python
def calculateGauss(x,mean,stdev):
    # x表示对应的属性值，mean是该属性在某一类别下的均值，stdev是标准差
    fenmu = math.exp(-math.pow((x-mean),2)/(2*math.pow(stdev,2)))
    fenzi = math.sqrt(2*math.pi)*stdev
    return fenmu/fenzi
# 得到的是高斯分布函数的计算结果
```

## 2.2 计算所属类的概率

上面计算的高斯分布是针对一个属性属于某一类的可能性，那么合并一个数据样本中所有属性的概率，最后便得到整个数据样本属于某个类的概率。
传入的是已知的均值标准差、测试样本，计算返回的是该样本属于每个类的可能性。


```python
def calculateClassProbabilities(summaries, sample):
    probabilities = {}
    for key,value in summaries.items():
        probabilities[key] = 1   #初始化为1，便于进行之后的乘法
        for i in range(len(value)):   # 每一个value代表的是一个列表，列表中的每一项是该属性的均值和方差
            mean,stdev = value[i]
            x = sample[i]
            temp = calculateGauss(x,mean,stdev)
            probabilities[key] *= temp
    return probabilities


sample = [14.12,1.48,2.32,16.8,95,2.2,2.43,.26,1.57,5,1.17,2.82,1280]
calculateClassProbabilities(summaries,sample)
```


    输出：{1.0: 4.447223997425311e-07,
     2.0: 4.484204868285748e-15,
     3.0: 9.048504947577875e-28}

## 2.3 单一预测

根据可能性大小，预测对应样本的所属类别


```python
def predict(summaries, sample):
    probabilities = calculateClassProbabilities(summaries,sample)
    bestLabel = None
    bestProbability = -1;
    for key,value in probabilities.items():
        if bestLabel is None or value>bestProbability:
            bestLabel = key
            bestProbability = value
    return bestLabel

sample = [14.12,1.48,2.32,16.8,95,2.2,2.43,.26,1.57,5,1.17,2.82,1280]
label = predict(summaries,sample)
print(label)
```

    输出：1.0


## 2.4 评估


### 2.4.1 多重预测

预测测试数据集中每个数据样本的预测，返回测试样本的预测列表。参数：summaries表示的是属性的特征（均值，标准差）。testData表示的是测试数据集。返回的是关于每一个测试数据的预测label。


```python
def getPredictions(summaries, testData):
    predictions = []
    for item in testData:
        temp = item[1:]
        label =  predict(summaries,temp)
        predictions.append(label)
    return predictions

predictions = getPredictions(summaries,testData)
```

### 2.4.2 计算精度

将预测值和测试数据集中的类别进行比较，计算得到一个介于0-1之间的精确率作为分类的精度。testData是测试集，predictions是预测的label值。返回预测准确的个数和精确率。


```python
def getAccuracy(testData, predictions):
    count = 0;
    for i in range(len(testData)):
        if(testData[i][0] == predictions[i]):
            count += 1;
    return (count, count/float(len(testData)))

count,accuracy = getAccuracy(testData, predictions)
print("testData is %d lines, predict accuracy is %d lines, the accuracy is %.4f" % (len(testData), count, accuracy))
```

    输出：testData is 59 lines, predict accuracy is 57 lines, the accuracy is 0.9661


# 3 合并代码


```python
import csv
import math
import random
import copy

filename = r'C:\Users\14259\Desktop\23周\wine.csv'


# 将读取文件内容封装成方法，存到二维数组中
def loadCsvFile(filename):
    csv_reader = csv.reader(open(filename,'r'))
    dataset = list(csv_reader)
#     print(dataset)
    # 将数据类型由字符串转换成浮点数
    for i in range(len(dataset)):
        dataset[i] = [float(x) for x in dataset[i]]
    return dataset

# 切分数据集为训练数据和测试数据
def splitDataset(dataset,splitRatio):
    # splitRation 是训练数据占得比例
    data= copy.deepcopy(dataset)
    length = len(dataset)
    trainSize= math.floor(length*splitRatio)
    trainData = []
    while(len(trainData)<trainSize):
        index = random.randint(0,len(data)-1)
        trainData.append(data.pop(index))
    testData = data
    return [trainData,testData]

# 将类别作为键，对应类别的值等作为改建对应的值
# 按照类别进行数据分类，返回一个对象，键为label值，值为属于该label的数据
def separateByClass(dataset):
    dataClass = {}
    for i in range(len(dataset)):
        item = dataset[i]  
        if(item[0] not in dataClass.keys()):  # 如果没有该键值
            dataClass[item[0]] = []
        dataClass[item[0]].append(item)
    return dataClass

# 计算均值
def mean(numbers):
    return sum(numbers) / float(len(numbers));

# 计算标准差
def stdev(numbers):
    avg = mean(numbers)
    variance = sum([pow(x-avg,2) for x in numbers])/(float(len(numbers)-1))  # 这里使用len(numbers)-1,可以参考百度百科【标准差】的说法
    return math.sqrt(variance)

# 提取数据特征，返回的是dataset数据的均值和标准差列表
def summrize(dataset):
    summaries = [(mean(attributes),stdev(attributes)) for attributes in zip(*dataset)]
    del summaries[0]   # 第一列表示的是类别列，不需要，可以删除
    return summaries

# 按照类别提取属性特征
def summrizeByClass(dataset):
    seperateData = separateByClass(dataset)      # 获取分类之后的数据
    summaries = {}
    for key,value in seperateData.items():
        summaries[key] = summrize(value)
    return summaries

# 计算高斯函数值，也就是某属性x在该类别下的概率
def calculateGauss(x,mean,stdev):
    # x表示对应的属性值，mean是该属性在某一类别下的均值，stdev是标准差
    fenmu = math.exp(-math.pow((x-mean),2)/(2*math.pow(stdev,2)))
    fenzi = math.sqrt(2*math.pi)*stdev
    return fenmu/fenzi

#  计算某一个数据多个属性属于label的可能性，返回的是一个对象，键值label，值是该数据属于该label的可能性
def calculateClassProbabilities(summaries, sample):
    probabilities = {}
    for key,value in summaries.items():
        probabilities[key] = 1   #初始化为1，便于进行之后的乘法
        for i in range(len(value)):   # 每一个value代表的是一个列表，列表中的每一项是该属性的均值和方差
            mean,stdev = value[i]
            x = sample[i]
            temp = calculateGauss(x,mean,stdev)
            probabilities[key] *= temp
    return probabilities

# 预测某一个样本的label
def predict(summaries, sample):
    probabilities = calculateClassProbabilities(summaries,sample)
    bestLabel = None
    bestProbability = -1;
    for key,value in probabilities.items():
        if bestLabel is None or value>bestProbability:
            bestLabel = key
            bestProbability = value
    return bestLabel

# 预测测试数据的分类
def getPredictions(summaries, testData):
    predictions = []
    for item in testData:
        temp = item[1:]
        label =  predict(summaries,temp)
        predictions.append(label)
    return predictions

# 准确率
def getAccuracy(testData, predictions):
    count = 0;
    for i in range(len(testData)):
        if(testData[i][0] == predictions[i]):
            count += 1;
    return (count, count/float(len(testData)))


def main():
    # 加载数据集
    dataset = loadCsvFile(filename)
    print("loaded data file {0} with {1} lines".format (filename,len(dataset)))
    # 切分数据
    trainData,testData = splitDataset(dataset,0.67)
    print("train data is %d lines and test data is %d lines" % (len(trainData),len(testData)))
    # 获取数据特征
    summaries = summrizeByClass(trainData)
    # 进行预测
    predictions = getPredictions(summaries,testData)
    # 计算准确率
    count,accuracy = getAccuracy(testData, predictions)
    print("testData is %d lines, predict accuracy is %d lines, the accuracy is %.4f" % (len(testData), count, accuracy))

main()

```

    输出：loaded data file C:\Users\14259\Desktop\23周\wine.csv with 178 lines
    train data is 119 lines and test data is 59 lines
    testData is 59 lines, predict accuracy is 56 lines, the accuracy is 0.9492


# 4 扩展


## 4.1 计算所属类的概率

上面在进行分类的时候，只是单一的计算对应可能性大小，没有计算所属类的概率。计算所属类的概率，只需要用属于当前类的可能性比上总的可能性大小即可。参数probabilities是可能性的相对大小，在前文中使用calculateClassProbabilities()函数中可以计算得到。


```python
def getProbabilities(probabilities):
    probs = {}
    allProb = 0
    for key,value in probabilities.items():
        allProb += value
    for key,value in probabilities.items():
        probs[key]  = '{:.4f}'.format(value/allProb)
    return probs

sample = [14.12,1.48,2.32,16.8,95,2.2,2.43,.26,1.57,5,1.17,2.82,1280]
probabilities = calculateClassProbabilities(summaries,sample)
getProbabilities(probabilities)
```


    输出：{1.0: '1.0000', 2.0: '0.0000', 3.0: '0.0000'}

## 4.2 思路拓展

- 对数概率。 对于一个给定的属性值，每个类的条件概率很小。当将它们相乘的时候结果会更小，那么存在浮点数溢出的可能性。一个常用的修复方案是合并其概率的对数值。（这个怎么合并）
- 不同密度函数。已经尝试了高斯朴素贝叶斯，可以尝试其他不同的分布。比如多项分布、伯努利分布或者内核朴素贝叶斯。

参考链接[这里](http://python.jobbole.com/81019/?f=geek#article-comment)
