---
title: 决策树分类代码实现
date: 2018-08-09 18:41:51
categories: 
- 数据挖掘
tags: ["决策树","分类"]
toc: true
comments: true
---

# 决策树

决策树算法是一种逼近离散函数值的方法。它是一种典型的分类方法，首先对数据进行处理，利用归纳算法生成可读的规则和决策树，然后使用决策对新数据进行分析。本质上决策树是通过一系列规则对数据进行分类的过程。

# 1 属性选择度量

属性选择度量是一种选择分类准则，把给定类标记的训练元祖的数据分区D“最好地”划分成单独类的启发式方法。

- 信息增益  ID3使用信息增益作为属性选择度量
- 增益率 ID3后继C4.5使用它作为信息增益的扩充，试图克服一种偏倚
- 基尼指数 在CART中使用，基尼指数考虑每个属性的二元划分

本文将使用信息增益作为属性的选择度量，下面是计算序列的信息熵的代码，参数attr_list代表的是该属性列表。


```python
from math import *;
import csv
import copy
from collections import defaultdict,namedtuple
filename = r'C:\Users\14259\Desktop\24周\electronics.csv';
feat_names = ['RID','age','income','student','credit_rating'];
```


```python
# 加载数据集
def load_data(filename):
    csv_reader = csv.reader(open(filename,'r'));
    dataset = list(csv_reader)
    return dataset

# 将数据集中属性和labels切分开
def split_lables(dataset):
    dataattrs = list(zip(*dataset))
    labels = dataattrs[-1];
    for item in dataset:
        del item[-1]
    return dataset,labels

dataset = load_data(filename);
print("data size is %d lines" % len(dataset))
dataset,labels = split_lables(dataset)
# print(dataset,labels)
```    


```python
# 计算信息熵
def get_shannon_entropy(attr_list):
    attrs = set(attr_list)   # 获取所有的属性值
    attr_nums = {key:attr_list.count(key) for key in attrs }  # 得到属性对应的数目
    attr_probs = [v/len(attr_list) for k,v in attr_nums.items()]   # 得到属性可能性列表
    entropy = sum([-p*log2(p) for p in attr_probs]);   # 计算信息熵
    return entropy

# attr_list = ["youth","youth","middle_aged","senior","senior","senior","middle_aged","youth","youth","senior","youth","middle_aged","middle_aged","senior"];
# attr_list = ["no",'no','yes','yes','yes','no','yes','no','yes','yes','yes','yes','yes','no'];
# get_shannon_entropy(attr_list)
```

## 1.1 使用信息增益选择属性


按照步骤，计算对应数据集的属性的信息增益，选择最合适的属性作为分类属性。首先需要计算该属性划分数据集之后得到的对应的子数据集和子类型列表，之后在进行相应的信息熵的计算。
- 划分数据集
- 计算信息增益，选择最佳属性


```python
def split_dataset(dataset, labels,feat_index):
    """ 根据某个特征划分数据集
    dataset：原始数据集，不包含标签
    labels: 对应的标签
    feat_index：特征在特征向量中的索引
    return  ：返回以feat_index作为分类属性的时候，该属性对应的取值的子数据集和对应的类型。
    
    """
    dataset = copy.deepcopy(dataset)   # 避免删除元素造成影响
    # 1、将每一列元素放在一个元组中，使用zip函数
    dataset_zip =list( zip(*dataset))

    #2、获取该特征列
    feat_col = dataset_zip[feat_index];
    
    
    #3、根据该列特征的所有取值，构建一个字典，键为属性的取值，值为对应的数据集。
    splited_dict = {};
    for item in set(feat_col):
        splited_dict[item] = [[],[]];
        
#     print(splited_dict)
    for i in range(len(feat_col)):    # 该特征列不能存在缺失，否则会有问题
        for key in splited_dict.keys():
            if feat_col[i] == key:
                del dataset[i][feat_index]      # 删除该特征列的信息，表示该特征列已经用过了。
                splited_dict[key][0].append(dataset[i])
                splited_dict[key][1].append(labels[i])
    return splited_dict
        
split_dataset(dataset,labels,1)
```

```python
# 根据信息增益选择属性
def choose_best_split_attr(dataset,labels):
    """dataset是数据集。
    """
#     # 1、从带标签的数据集中取出标签列
#     labels = list(zip(*dataset))[-1]

    # 2、计算labels中分类所需要的期望信息
    entropy_all = get_shannon_entropy(labels)
#     print(entropy_all)
  
    # 3、计算每个属性的期望信息需求
    entropys = {};   # 存储对应的entropys
    for i in range(1,len(dataset[0])):    # 注意这里是使用1开始的，因为如果从0开始，那么第一个序号属性会使最好的分类属性，
            # 因为它将每一个数据都划分成一个类，这显然是没有意义的，这也是偏倚出现的原因，可以使用增益率作为属性选择计算方法来避免。
        # 划分数据集，返回的是一个字典
        split_data = split_dataset(dataset,labels,i)
        entropys[i] = [];
        for k,v in split_data.items():
            # split_data字典中，键是属性值，值=[子数据集，子类型列表]
            k_rate = len(v[0])/len(labels)    # 该属性对应的数据集的个数占数据集总个数的比例
            entropy = get_shannon_entropy(v[1])   # 计算该属性对应的数据标签的信息熵。
            temp = k_rate*entropy;
            entropys[i].append(temp)
        entropys[i] = sum(entropys[i])    # 计算属性i进行划分的时候的期望信息
#     print(entropys)
    return min(entropys,key=entropys.get)
    
choose_best_split_attr(dataset,labels)
```


```python
# 比如以第三个属性作为特征值进行手动的计算entropy
result = (7/14)*(-(4/7)*log2(4/7)-(3/7)*log2(3/7))+(7/14)*(-(6/7)*log2(6/7)-(1/7)*log2(1/7))
result     # 与上面计算的结果一致，可以看出算法正确
```

# 2 树分裂

有了选取最佳分裂属性的算法，接着就开始使用选择的属性来将树进一步的分裂。所谓树的分裂只不过是根据选择的属性将数据集划分，然后在总划分出来的数据集中再次调用选取属性的方法选择子数据集的最佳属性，最好的实现方式就是递归了。  
python中使用什么数据结构表示决策树？可以使用字典很方便的表示决策树的嵌套，一个树的根节点便是属性，属性对应的值又是一个新的字典，其中key为属性的可能值，value为子树。

## 2.1 得到占据大多数的类型

当所有属性都使用完的时候还没有将数据完全分开，此时就返回所占比重较大的那个分类。比如使用完最后一个属性，得到的labels列表为['yes','yes','no']那么返回的就是yes类型。


```python
def get_majority(labels):
    label_nums = defaultdict(lambda:0);
    for label in labels:
        label_nums[label] += 1;
    return max(label_nums, key=label_nums.get)

# labels = ['yes','yes','no'];
# print(get_majority(labels));
```

## 2.2 创建树

树分裂终止的两个条件是：
- 遍历完所有的属性。在进行树分裂的时候，我们数据集中数据向量的属性是不断缩短的，当缩短到1的时候（数据向量中包括labels，所以这里为1.否则为0），说明数据集中的属性已被全部使用完毕，便不能再分裂下去了，此时我们选取最终子数据集中的众数作为最终的分类结果放在叶子节点上。
- 新划分的数据集中只有一个属性。所该节点下面的子数据集中labels是一致的，那么就不用在进行分类了。


```python
def create_tree(dataset, labels, feat_names):
    """dataset是含有标签的数据集，labels是对应的标签，feat_names是数据集中数据相应的特征属性
    """
    global Tree;
    if (len(set(labels)) == 1):
        # 如果数据集中只有一个类型，就可以停止分裂
        return labels[0]

    if len(feat_names) == 1:  # 因为没有考虑第一个属性RID
        # 如果属性已经用完了，那么返回比例最多的类型。
        return get_majority(labels)

    tree = {};
    best_feature_index = choose_best_split_attr(dataset, labels);  # 获取最好的分类属性的index
    feature = feat_names[best_feature_index];  # 获取属性index对应的属性名
    tree[feature] = {};

    sub_feat_names = feat_names[:];
    #     print(sub_feat_names[best_feature_index])
    sub_feat_names.pop(best_feature_index);

    splited_dict = split_dataset(dataset, labels, best_feature_index);  # 根据该属性分类，得到分类之后的数据集
    for feat_col, (sub_dataset, sub_labels) in splited_dict.items():
        tree[feature][feat_col] = create_tree(sub_dataset, sub_labels, sub_feat_names)

    return tree;

create_tree(dataset,labels,feat_names)
```




    {'age': {'middle_aged': 'yes',
      'senior': {'credit_rating': {'excellent': 'no', 'fair': 'yes'}},
      'youth': {'student': {'no': 'no', 'yes': 'yes'}}}}



# 3 合并所有代码


```python
from math import *;
import csv
import copy
from collections import defaultdict

filename = r'C:\Users\14259\Desktop\24周\electronics.csv';
feat_names = ['RID', 'age', 'income', 'student', 'credit_rating'];

# 加载数据集
def load_data(filename):
    csv_reader = csv.reader(open(filename, 'r'));
    dataset = list(csv_reader)
    return dataset


# 将数据集中属性和labels切分开
def split_lables(dataset):
    dataattrs = list(zip(*dataset))
    labels = dataattrs[-1];
    for item in dataset:
        del item[-1]
    return dataset, labels


# 计算信息熵
def get_shannon_entropy(attr_list):
    attrs = set(attr_list)  # 获取所有的属性值
    attr_nums = {key: attr_list.count(key) for key in attrs}  # 得到属性对应的数目
    attr_probs = [v / len(attr_list) for k, v in attr_nums.items()]  # 得到属性可能性列表
    entropy = sum([-p * log2(p) for p in attr_probs]);  # 计算信息熵
    return entropy


def split_dataset(dataset, labels, feat_index):
    """ 根据某个特征划分数据集
    dataset：原始数据集，不包含标签
    labels: 对应的标签
    feat_index：特征在特征向量中的索引
    return  ：返回以feat_index作为分类属性的时候，该属性对应的取值的子数据集和对应的类型。

    """
    dataset = copy.deepcopy(dataset)  # 避免删除元素造成影响
    # 1、将每一列元素放在一个元组中，使用zip函数
    dataset_zip = list(zip(*dataset))

    # 2、获取该特征列
    feat_col = dataset_zip[feat_index];

    # 3、根据该列特征的所有取值，构建一个字典，键为属性的取值，值为对应的数据集。
    splited_dict = {};
    for item in set(feat_col):
        splited_dict[item] = [[], []];

    #     print(splited_dict)
    for i in range(len(feat_col)):  # 该特征列不能存在缺失，否则会有问题
        for key in splited_dict.keys():
            if feat_col[i] == key:
                del dataset[i][feat_index]  # 删除该特征列的信息，表示该特征列已经用过了。
                splited_dict[key][0].append(dataset[i])
                splited_dict[key][1].append(labels[i])
    return splited_dict


# 根据信息增益选择属性
def choose_best_split_attr(dataset, labels):
    """dataset是数据集。
    """
    #     # 1、从带标签的数据集中取出标签列
    #     labels = list(zip(*dataset))[-1]

    # 2、计算labels中分类所需要的期望信息
    entropy_all = get_shannon_entropy(labels)
    #     print(entropy_all)

    # 3、计算每个属性的期望信息需求
    entropys = {};  # 存储对应的entropys
    for i in range(1, len(dataset[0])):  # 注意这里是使用1开始的，因为如果从0开始，那么第一个序号属性会使最好的分类属性，
        # 因为它将每一个数据都划分成一个类，这显然是没有意义的，这也是偏倚出现的原因，可以使用增益率作为属性选择计算方法来避免。
        # 划分数据集，返回的是一个字典
        split_data = split_dataset(dataset, labels, i)
        entropys[i] = [];
        for k, v in split_data.items():
            # split_data字典中，键是属性值，值=[子数据集，子类型列表]
            k_rate = len(v[0]) / len(labels)  # 该属性对应的数据集的个数占数据集总个数的比例
            entropy = get_shannon_entropy(v[1])  # 计算该属性对应的数据标签的信息熵。
            temp = k_rate * entropy;
            entropys[i].append(temp)
        entropys[i] = sum(entropys[i])  # 计算属性i进行划分的时候的期望信息
    #     print(entropys)
    return min(entropys, key=entropys.get)


def get_majority(labels):
    label_nums = defaultdict(lambda: 0);
    for label in labels:
        label_nums[label] += 1;
    return max(label_nums, key=label_nums.get)


def create_tree(dataset, labels, feat_names):
    """dataset是含有标签的数据集，labels是对应的标签，feat_names是数据集中数据相应的特征属性
    """
    global Tree;
    if (len(set(labels)) == 1):
        # 如果数据集中只有一个类型，就可以停止分裂
        return labels[0]

    if len(feat_names) == 1:  # 因为没有考虑第一个属性RID
        # 如果属性已经用完了，那么返回比例最多的类型。
        return get_majority(labels)

    tree = {};
    best_feature_index = choose_best_split_attr(dataset, labels);  # 获取最好的分类属性的index
    feature = feat_names[best_feature_index];  # 获取属性index对应的属性名
    tree[feature] = {};

    sub_feat_names = feat_names[:];
    #     print(sub_feat_names[best_feature_index])
    sub_feat_names.pop(best_feature_index);

    splited_dict = split_dataset(dataset, labels, best_feature_index);  # 根据该属性分类，得到分类之后的数据集
    for feat_col, (sub_dataset, sub_labels) in splited_dict.items():
        tree[feature][feat_col] = create_tree(sub_dataset, sub_labels, sub_feat_names)

    return tree;


def main():
    dataset = load_data(filename);
    print("data size is %d lines" % len(dataset))
    dataset, labels = split_lables(dataset)
    tree = create_tree(dataset, labels, feat_names)
    print("生成的决策树如下:")
    print(tree)


main()
```

    data size is 14 lines
    生成的决策树如下:
    {'age': {'senior': {'credit_rating': {'excellent': 'no', 'fair': 'yes'}}, 'youth': {'student': {'yes': 'yes', 'no': 'no'}}, 'middle_aged': 'yes'}}
    

# 4 可视化决策树

通过嵌套字典表示决策树对人来说不好理解，可以借助可视化工具将该结构可视化。使用Graphviz来可视化树结构。因此，首先需要将字典表示的树生成 Graphviz Dot文件内容的函数，思想就是递归获取整棵树的所有节点和连接节点的边然后将这些节点和边生成Dot格式的字符串写入到文件中，然后进行图形的绘制。
__这一部分内容暂时没有做，需要安装Graphviz软件以及学习使用。__

# 5 使用生成的决策树进行分类

对未知数据进行预测，主要是根据树中的结点递归找到叶子结点即可。


```python
def classify(data_vect,feat_names,tree):
    if(type(tree) is not dict):
        # 说明找到了叶子结点
        return tree;  
    
    feature = list(tree.keys())[0]   # 获取树的最顶层的属性
    value = data_vect[feat_names.index(feature)]  #获取测试数据该属性对应的值
    sub_tree = tree[feature][value]
    return classify(data_vect,feat_names,sub_tree)
# data_vect = [14,'senior','medium','no','fair'];
# label = classify(data_vect,feat_names,tree);
# print(label)
```

    yes
    


```python
def predict_accuracy(dataset,labels,feat_names,tree):
    predict_labels = [];
    for data_vect in dataset:
        label = classify(data_vect,feat_names,tree)
        predict_labels.append(label)
    
    count = 0;
    labels =list(zip(predict_labels,labels))
    for label in labels:
        if label[0] == label[1]:
            count += 1;
            
    return count/len(dataset)

dataset =[['1', 'youth', 'high', 'no', 'fair'], ['2', 'youth', 'high', 'no', 'excellent'], ['3', 'middle_aged', 'high', 'no', 'fair'], ['4', 'senior', 'medium', 'no', 'fair'], ['5', 'senior', 'low', 'yes', 'fair'], ['6', 'senior', 'low', 'yes', 'excellent']]
labels = ['no', 'no', 'no', 'no', 'yes', 'no', 'yes']
accuracy = predict_accuracy(dataset,labels,feat_names,tree)

print("准确率是: %.4f" % accuracy)
```

    准确率是: 0.6667
    

# 6 总结

本文一步步实现了一个基本的决策树分类算法，里面还有较多不完善的地方。
- 比如在属性选择方面，使用的是信息增益，可能存在偏倚问题。之后可以通过增益率作为属性选择度量。
- 如果测试数据中出现了训练数据中没有出现的某个属性的类别，那么会出现错误。
- 本文没有涉及到树剪枝等问题，可能是数据集比较简单，没有遇到该类问题。  
写本文主要是为了熟悉决策树的一个分类的原理，从而可以更好的理解，之后在进行使用的时候可以使用目前比较完善的，比如scikit-learn
