---
title: EM算法
date: 2018-09-13 20:41:56
categories:
- Algorithm
tags:
- EM
- Algorithm
mathjax: true
---


# 引言
EM(Expectation Maximization Algorithm)最大期望算法，参数是最大似然估计。EM会涉及一些数学知识，比如最大似然估计和Jensen不等式。本文主要包含以下内容：
- 最大似然估计
- Jensen不等式

# 1 最大似然估计(Maximum Likelihood Estimation)
最大似然估计是一种统计方法，用来求一个样本集的相关概率密度函数的参数。

假设我们有一个随机样本序列*X1,X2,...Xn*, 每一个*Xi*的概率（质量）分布函数是 $f(x_i; \theta)$，因此，$X_1, X_2,...X_n$的联合概率密度（质量）函数$L(\theta)$： 

$$
L(\theta) = P(X_1=x_1,X_2 = x_2,...X_n=x_n) \\ 
= f(x_1;\theta)f(x_2;\theta)...f(x_n,\theta) = \prod_{i=1}^Nf(x_i;\theta)
$$

上面的公式中，第一个等号是联合概率密度函数的定义，第二个等号是因为我们定义了$X_i$是独立的。现在，根据最大似然估计的思想，一个合理的方式是将$L(\theta)$看做是$\theta$的函数，并寻找一个$\theta$让$L(\theta)$的值最大。

## 1.1 举例说明
假设，我们有一个随机样本$X_1,X_2,...X_n$，其中:
- $X_i=0$表示随机选择的一个学生没有一辆跑车
- $X_i=1$表示随机选择的一个学生拥有一辆跑车  

假设$X_i$是在未知参数$p$下的独立伯努利随机变量，求得$p$的最大似然估计，即拥有跑车的学生占得比例。也就是说，目前已经知道样本的值$X_1,X_2,...X_n$，同时每一个样本符合伯努利分布，目标是在这种样本出现的时候，拥有跑车的学生占的比例最可能的值。详细内容可以参考[here](https://onlinecourses.science.psu.edu/stat414/node/191/)

### 另一个例子

假设随机选择的学生体重符合未知均值$\mu$和标准差$\sigma$正态分布，随机抽样的10名学生的体重值如下： 115  122  130  127  149  160  152  138  149  180，求$\mu$的最大似然估计，即学生的平均体重。

首先，$X_i$的概率密度函数是：

$$
f(x_i;\mu,\sigma^2) = \frac{1}{\sigma \sqrt{2\pi}}exp\left[-\frac{(x_i-\mu)^2}{2\sigma^2}\right]
$$  
之后，求联合概率密度函数，也即是似然函数：（具体的公式推导就不展开了）

$$
L(\mu, \sigma) = \sigma^{-n}(2\pi)^{-n/2}exp\left[-\frac{1}{2\sigma^2}\sum_{i=1}^n(x_i-\mu)^2\right]
$$
通过一些处理方法，比如对$L(\theta)$求log函数，然后再对$\mu$求偏导，并令偏导数为0，得到$\mu$的最大似然估计如下:

$$
\hat\mu = \frac{1}{n}\sum_{i=1}^{n}X_i = \bar{X}
$$
然后，基于给定的样本数据，得到$\mu$的最大似然估计如下:

$$
\hat\mu = \frac{1}{n}\sum_{i=1}^{n}x_i = \frac 1 {10}(115+122+...+180) = 142.2
$$


# 2 Jensen不等式
Jensen不等式是一个涉及函数凸性质的不等式，首先有如下定义：
- 如果在函数图像在区间$I$上的任意两点之间的线段位于图形的上方，那么该函属在区间$I$上是凸的，比如函数$f(x) = x^2$
- 如果在函数图像在区间$I$上的任意两点之间的线段位于图形的下方，那么该函属在区间$I$上是凹的，比如函数$f(x) = -x^2$

## 2.1 Jensen不等式呈现形式
当函数$f$在区间$I$上是凸函数的时候，并且$\omega_1,\omega_2...\omega n \geq 0$，则有：

$$
\frac{\omega_1f(x_1)+\omega_2f(x_2)+...+\omega_nf(x_n)}{\omega_1+\omega_2+...+\omega_n} \geq f\left(\frac{\omega_1x_1+\omega_2x_2+...+\omega_nx_n}{\omega_1+\omega_2+...+\omega_n}\right)
$$
当$\omega_1=\omega_2=...=\omega_n=1$的时候，可以转化成下图中的特殊情况：

$$
\frac{f(x_1)+f(x_2)+...+f(x_n)}{n} \geq f\left(\frac{x_1+x_2+...+x_n}{n}\right)
$$

至于该公式的具体证明，这里就略过了

# 3 EM算法
在已经了解上最大似然估计和Jensen不等式的条件下，开始学习一下EM算法。

## 3.1 EM算法简单介绍
EM算法已经成为不完整数据的统计估计问题的通用工具，或者是可以以类似形式提出的问题，比如混合统计。EM算法是在存在丢失或者隐藏数据的情况下计算最大似然(ML)估计的有效迭代过程，在最大似然估计中，我们希望得到这个观察数据最可能出现时的模型参数。

概率模型有时既含有观测变量(observable variable)，又含有隐变量或潜在变量（latent variable），如果仅有观测变量，那么给定数据就能用极大似然估计或贝叶斯估计来估计model参数；但是当模型含有隐变量时，需要一种含有隐变量的概率模型参数估计的极大似然方法估计——EM算法

EM算法由两个步骤组成，分别是E-step和M-step：
- E-step:利用对隐藏变量的现有的估计值，计算其最大似然估计值，使用条件期望来实现
- M-step：假设这些隐藏数据是已知的，最大化似然函数。

之后使用M-step上找到的参数估计值运用到下一个E-step中，此过程不断迭代。

## 3.2 一个例子
首先，假设我们有两枚硬币，$C_1,C_2$，假设抛硬币$C_1$，头在上的概率为$\theta_1$，假设抛硬币$C_2$，头在上的概率为$\theta_2$，我们需要根据一些试验的结果去得到$\theta_1,\theta_2$的值。

第一次实验，我们选择了5次硬币，每一个硬币做了10次实验，得到的结果如下图：  
<img src="/images/数据挖掘理论与算法/EM/EM_01.png" width="300px" height="300px">

将数据进行统计，得到下面的结果：  
<img src="/images/数据挖掘理论与算法/EM/EM_02.png" width="200px" height="300px">

之后使用这些数据可以估计$\theta_1,\theta_2$的值，计算结果如下：  

$$
\theta_1 = \frac{使用硬币C_1得到的头朝上的次数}{总共使用硬币C_1的次数} = \frac{24}{24+6} = 0.8
$$
同理可以计算$\theta_2$的值为9/(9+11) = 0.45  


上面这个问题是我们知道每次使用的硬币，可以使用最大似然估计计算得到结果。如果我们不知道每次使用的硬币，也就是存在隐变量的时候，只是有上面图中的抛硬币的结果。也就是我们不知道每次抛掷的时候的硬币，而是将它们看为是隐藏变量。此时就需要使用EM算法来解决。

首先，我们初始化隐藏变量的值$\theta_A=0.6,\theta_B=0.5$，也就是知道了抛A硬币的时候，头朝上的可能性是0.6，抛B硬币的时候，头朝上的可能性是0.5，那么现在已经知道了一组数据 H T T T H H T H T H，也就是知道了结果，判断该硬币是A硬币和B硬币的可能性。可以这样理解：  
A盒子中有10个球，其中6个黑球，4个红球；B盒子中有10个球，5个黑球和5个红球，现在从某个盒子中取10次，得到5个黑球和5个红球，来自于A，B盒子的概率。  
若是从A盒子中取球，则有：

$$
\frac{C_{10}^50.6^50.4^5}{C_{10}^50.6^50.4^5+C_{10}^50.5^50.5^5} = 0.44914
$$
这里的$C_{10}^5$可以去掉，因为给定了球的顺序，若H代表黑球，T代表红球，那么就可以理解为，依次取出黑球、红球、红球、红球、黑球...。
另一种理解：若使用A硬币，那么出现H T T T H H T H T H的时候，也就是第一次出现H，概率为0.6，第二次出现T，概率为0.4，依次进行累乘，得到 $0.6^5*0.4^5$，同理可以得出使用硬币B的时候，出现这种结果的概率，然后分别计算出A和B的可能性就可以了【没有使用贝叶斯公式】

之后根据计算出来的A，B硬币的可能性和A，B硬币头朝上的概率，在抛10次的情况下，计算出新的结果，如下图：

<img src="/images/数据挖掘理论与算法/EM/EM_03.png" width="400px" height="300px">

之后根据这个结果，计算出A，B硬币头朝上的概率，然后更新$\theta_A,\theta_B$，进行第二次迭代。

本文只是针对EM知识做了一个整体上的理解，关于具体的公式推导以及如何应用到其他场景还存在不足，之后需要逐渐进行补充。同时关于下面的链接中，由于统计学知识的缺陷，下面的EM算法中的推导不怎么理解。

# 参考
- https://www.cnblogs.com/Gabby/p/5344658.html
- https://www.cnblogs.com/xieyue/p/4384915.html 
- https://onlinecourses.science.psu.edu/stat414/node/191/ [MLE]
- https://www.cs.utah.edu/~piyush/teaching/EM_algorithm.pdf [EM]
- https://ibug.doc.ic.ac.uk/media/uploads/documents/expectation_maximization-1.pdf [EM recommend]
- https://yq.aliyun.com/articles/290862 [EM]
- https://www.cnblogs.com/sxzhou/p/8551547.html[从公式推导上理解EM算法]
- https://blog.csdn.net/u011954647/article/details/53205664[The EM algorithm and extensions][这是一篇中文的博客，里面是EM算法的详细讲解]